{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6C91Y7rbhb_"
   },
   "source": [
    "# SAAB-v3 Training on Google Colab\n",
    "\n",
    "This notebook provides an interface to train SAAB-v3 models on Google Colab with GPU support.\n",
    "\n",
    "## Setup & Installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lmk346URbhcB"
   },
   "source": [
    "### Cell 1: Installation and Dependencies\n",
    "\n",
    "Install PyTorch with CUDA support and all project dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42897,
     "status": "ok",
     "timestamp": 1767554120153,
     "user": {
      "displayName": "Mira Ali",
      "userId": "17000461928602694792"
     },
     "user_tz": -180
    },
    "id": "x3ajnPqGbhcB",
    "outputId": "c91e74b2-a2c8-4dda-a637-31c15ed07348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.6.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "CUDA version: 12.6\n",
      "Mounted at /content/drive\n",
      "\n",
      "Google Drive mounted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch with CUDA support for Colab\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install project dependencies\n",
    "!pip install pandas numpy pydantic networkx tqdm scikit-learn matplotlib pyyaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1405,
     "status": "ok",
     "timestamp": 1767555188464,
     "user": {
      "displayName": "Mira Ali",
      "userId": "17000461928602694792"
     },
     "user_tz": -180
    },
    "id": "MIrwQaWNf5rG",
    "outputId": "579cfd16-eb4c-424b-bc18-449c1c099a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "CUDA version: 12.6\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "\n",
      "Google Drive mounted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Mount Google Drive for data/checkpoint persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"\\nGoogle Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOqo9s7TbhcC"
   },
   "source": [
    "### Cell 2: Project Upload\n",
    "\n",
    "Upload your project ZIP file and extract it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Un7nMAyrbhcD"
   },
   "source": [
    "### Cell 3: Path Setup\n",
    "\n",
    "Set up Google Drive paths for data, checkpoints, and logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1767555192622,
     "user": {
      "displayName": "Mira Ali",
      "userId": "17000461928602694792"
     },
     "user_tz": -180
    },
    "id": "XrcZwFccbhcD",
    "outputId": "7a05857c-d2e8-4cdd-afc4-28366ced9cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Drive directory structure:\n",
      "  Dataset: /content/drive/MyDrive/SAAB/dataset/raw\n",
      "  Artifacts: /content/drive/MyDrive/SAAB/dataset/artifacts\n",
      "  Checkpoints: /content/drive/MyDrive/SAAB/checkpoints\n",
      "  Logs: /content/drive/MyDrive/SAAB/logs\n",
      "\n",
      "✓ Directories created/verified\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Google Drive base path\n",
    "drive_base = Path(\"/content/drive/MyDrive/SAAB\")\n",
    "\n",
    "# Create directory structure in Drive\n",
    "directories = [\n",
    "    drive_base / \"dataset\" / \"raw\",\n",
    "    drive_base / \"dataset\" / \"artifacts\",\n",
    "    drive_base / \"checkpoints\",\n",
    "    drive_base / \"logs\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Google Drive directory structure:\")\n",
    "print(f\"  Dataset: {drive_base / 'dataset' / 'raw'}\")\n",
    "print(f\"  Artifacts: {drive_base / 'dataset' / 'artifacts'}\")\n",
    "print(f\"  Checkpoints: {drive_base / 'checkpoints'}\")\n",
    "print(f\"  Logs: {drive_base / 'logs'}\")\n",
    "print(\"\\n✓ Directories created/verified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVeSNjubbhcE"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Set your training parameters below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRRnb6CsbhcE"
   },
   "source": [
    "### Cell 4: Training Configuration\n",
    "\n",
    "Edit these variables to configure your training run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1767557423009,
     "user": {
      "displayName": "Mira Ali",
      "userId": "17000461928602694792"
     },
     "user_tz": -180
    },
    "id": "8zZsaQW6bhcE",
    "outputId": "a06d824e-30ff-4363-f0ed-e707bad6a4ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Dataset: dbpedia\n",
      "  Model Type: flat\n",
      "  Experiment Name: dbpedia_flat\n",
      "  Resume from: None (starting fresh)\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "dataset_name = \"dbpedia\"  # Dataset identifier\n",
    "model_type = \"flat\"     # \"flat\", \"scratch\", or \"saab\"\n",
    "experiment_name = None      # Optional: custom experiment name (defaults to {dataset_name}_{model_type})\n",
    "resume_checkpoint = None    # Optional: path to checkpoint file for resuming training\n",
    "\n",
    "# Auto-generate experiment name if not provided\n",
    "if experiment_name is None:\n",
    "    experiment_name = f\"{dataset_name}_{model_type}\"\n",
    "\n",
    "# Display configuration\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Dataset: {dataset_name}\")\n",
    "print(f\"  Model Type: {model_type}\")\n",
    "print(f\"  Experiment Name: {experiment_name}\")\n",
    "if resume_checkpoint:\n",
    "    print(f\"  Resume from: {resume_checkpoint}\")\n",
    "else:\n",
    "    print(f\"  Resume from: None (starting fresh)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWqaKyW5bhcE"
   },
   "source": [
    "## Training\n",
    "\n",
    "Execute the training command.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6qkY1NnbhcE"
   },
   "source": [
    "### Cell 5: Run Training\n",
    "\n",
    "This cell executes the training CLI command. Output will be displayed in real-time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 627025,
     "status": "ok",
     "timestamp": 1767558108638,
     "user": {
      "displayName": "Mira Ali",
      "userId": "17000461928602694792"
     },
     "user_tz": -180
    },
    "id": "vwD5YoxXbhcE",
    "outputId": "56d0066e-0368-426e-c1fb-5b8f61483724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Pydantic defaults for configuration...\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "Preprocessing:\n",
      "  - vocab_size: 30000\n",
      "  - max_seq_len: 512\n",
      "  - device: cuda\n",
      "\n",
      "Model:\n",
      "  - d_model: 768\n",
      "  - num_layers: 4\n",
      "  - num_heads: 6\n",
      "  - dropout: 0.2\n",
      "  - device: cuda\n",
      "\n",
      "Training:\n",
      "  - learning_rate: 1e-06\n",
      "  - batch_size: 64\n",
      "  - num_epochs: None\n",
      "  - lr_schedule: reduce_on_plateau\n",
      "  - max_grad_norm: 0.1\n",
      "  - early_stopping_patience: 3\n",
      "  - device: cuda\n",
      "============================================================\n",
      "\n",
      "Fitting preprocessor on training data from /content/drive/MyDrive/SAAB/dataset/raw/dbpedia/train.csv...\n",
      "\n",
      "Extracting tokens:   0%|          | 0/560000 [00:00<?, ?it/s]\n",
      "Extracting tokens:   0%|          | 1908/560000 [00:00<00:40, 13790.16it/s]\n",
      "Extracting tokens:   1%|          | 5937/560000 [00:00<00:20, 27250.15it/s]\n",
      "Extracting tokens:   2%|▏         | 8875/560000 [00:00<00:25, 21618.82it/s]\n",
      "Extracting tokens:   2%|▏         | 12696/560000 [00:00<00:27, 19693.87it/s]\n",
      "Extracting tokens:   3%|▎         | 16734/560000 [00:00<00:21, 24834.08it/s]\n",
      "Extracting tokens:   4%|▎         | 20185/560000 [00:00<00:26, 20492.77it/s]\n",
      "Extracting tokens:   4%|▍         | 24240/560000 [00:01<00:21, 24882.99it/s]\n",
      "Extracting tokens:   5%|▌         | 28313/560000 [00:01<00:18, 28656.47it/s]\n",
      "Extracting tokens:   6%|▌         | 31605/560000 [00:01<00:24, 21258.88it/s]\n",
      "Extracting tokens:   6%|▋         | 35590/560000 [00:01<00:20, 25071.22it/s]\n",
      "Extracting tokens:   7%|▋         | 39619/560000 [00:01<00:18, 28528.04it/s]\n",
      "Extracting tokens:   8%|▊         | 42981/560000 [00:01<00:25, 20159.65it/s]\n",
      "Extracting tokens:   8%|▊         | 47021/560000 [00:02<00:21, 24036.48it/s]\n",
      "Extracting tokens:   9%|▉         | 51083/560000 [00:02<00:18, 27599.03it/s]\n",
      "Extracting tokens:  10%|▉         | 55060/560000 [00:02<00:16, 30455.76it/s]\n",
      "Extracting tokens:  10%|█         | 58625/560000 [00:02<00:25, 19868.30it/s]\n",
      "Extracting tokens:  11%|█         | 62685/560000 [00:02<00:21, 23668.47it/s]\n",
      "Extracting tokens:  12%|█▏        | 66775/560000 [00:02<00:18, 27243.53it/s]\n",
      "Extracting tokens:  13%|█▎        | 70770/560000 [00:02<00:16, 30147.55it/s]\n",
      "Extracting tokens:  13%|█▎        | 74389/560000 [00:03<00:26, 18257.69it/s]\n",
      "Extracting tokens:  14%|█▍        | 78471/560000 [00:03<00:21, 22065.01it/s]\n",
      "Extracting tokens:  15%|█▍        | 82522/560000 [00:03<00:18, 25646.81it/s]\n",
      "Extracting tokens:  15%|█▌        | 86546/560000 [00:03<00:16, 28815.40it/s]\n",
      "Extracting tokens:  16%|█▌        | 90630/560000 [00:03<00:14, 31660.77it/s]\n",
      "Extracting tokens:  17%|█▋        | 94703/560000 [00:03<00:13, 33952.73it/s]\n",
      "Extracting tokens:  18%|█▊        | 98557/560000 [00:04<00:26, 17747.68it/s]\n",
      "Extracting tokens:  18%|█▊        | 102546/560000 [00:04<00:21, 21301.98it/s]\n",
      "Extracting tokens:  19%|█▉        | 106591/560000 [00:04<00:18, 24874.03it/s]\n",
      "Extracting tokens:  20%|█▉        | 110642/560000 [00:04<00:15, 28163.54it/s]\n",
      "Extracting tokens:  20%|██        | 114691/560000 [00:04<00:14, 31011.22it/s]\n",
      "Extracting tokens:  21%|██        | 118750/560000 [00:04<00:13, 33387.03it/s]\n",
      "Extracting tokens:  22%|██▏       | 122790/560000 [00:04<00:12, 35222.43it/s]\n",
      "Extracting tokens:  23%|██▎       | 126705/560000 [00:05<00:26, 16244.39it/s]\n",
      "Extracting tokens:  23%|██▎       | 130735/560000 [00:05<00:21, 19809.71it/s]\n",
      "Extracting tokens:  24%|██▍       | 134741/560000 [00:05<00:18, 23347.83it/s]\n",
      "Extracting tokens:  25%|██▍       | 138775/560000 [00:05<00:15, 26741.32it/s]\n",
      "Extracting tokens:  26%|██▌       | 142857/560000 [00:05<00:13, 29866.28it/s]\n",
      "Extracting tokens:  26%|██▌       | 146910/560000 [00:05<00:12, 32432.89it/s]\n",
      "Extracting tokens:  27%|██▋       | 150883/560000 [00:05<00:11, 34299.40it/s]\n",
      "Extracting tokens:  28%|██▊       | 154969/560000 [00:06<00:11, 36052.46it/s]\n",
      "Extracting tokens:  28%|██▊       | 159034/560000 [00:06<00:10, 37323.51it/s]\n",
      "Extracting tokens:  29%|██▉       | 163021/560000 [00:06<00:26, 14797.33it/s]\n",
      "Extracting tokens:  30%|██▉       | 167038/560000 [00:06<00:21, 18247.49it/s]\n",
      "Extracting tokens:  31%|███       | 171111/560000 [00:07<00:17, 21903.76it/s]\n",
      "Extracting tokens:  31%|███▏      | 175131/560000 [00:07<00:15, 25353.40it/s]\n",
      "Extracting tokens:  32%|███▏      | 179192/560000 [00:07<00:13, 28590.02it/s]\n",
      "Extracting tokens:  33%|███▎      | 183204/560000 [00:07<00:12, 31271.40it/s]\n",
      "Extracting tokens:  33%|███▎      | 187162/560000 [00:07<00:11, 33318.69it/s]\n",
      "Extracting tokens:  34%|███▍      | 191197/560000 [00:07<00:10, 35164.36it/s]\n",
      "Extracting tokens:  35%|███▍      | 195216/560000 [00:07<00:09, 36533.09it/s]\n",
      "Extracting tokens:  36%|███▌      | 199201/560000 [00:07<00:09, 37462.62it/s]\n",
      "Extracting tokens:  36%|███▋      | 203166/560000 [00:08<00:27, 12918.18it/s]\n",
      "Extracting tokens:  37%|███▋      | 207230/560000 [00:08<00:21, 16291.69it/s]\n",
      "Extracting tokens:  38%|███▊      | 211319/560000 [00:08<00:17, 19934.41it/s]\n",
      "Extracting tokens:  38%|███▊      | 215388/560000 [00:08<00:14, 23558.93it/s]\n",
      "Extracting tokens:  39%|███▉      | 219463/560000 [00:08<00:12, 26989.60it/s]\n",
      "Extracting tokens:  40%|███▉      | 223519/560000 [00:09<00:11, 30000.81it/s]\n",
      "Extracting tokens:  41%|████      | 227594/560000 [00:09<00:10, 32587.71it/s]\n",
      "Extracting tokens:  41%|████▏     | 231656/560000 [00:09<00:09, 34642.32it/s]\n",
      "Extracting tokens:  42%|████▏     | 235735/560000 [00:09<00:08, 36287.27it/s]\n",
      "Extracting tokens:  43%|████▎     | 239750/560000 [00:09<00:08, 37354.09it/s]\n",
      "Extracting tokens:  44%|████▎     | 243803/560000 [00:09<00:08, 38252.49it/s]\n",
      "Extracting tokens:  44%|████▍     | 247868/560000 [00:09<00:08, 38942.29it/s]\n",
      "Extracting tokens:  45%|████▍     | 251946/560000 [00:09<00:07, 39477.07it/s]\n",
      "Extracting tokens:  46%|████▌     | 256035/560000 [00:09<00:07, 39890.05it/s]\n",
      "Extracting tokens:  46%|████▋     | 260095/560000 [00:10<00:25, 11619.91it/s]\n",
      "Extracting tokens:  47%|████▋     | 264136/560000 [00:10<00:20, 14760.09it/s]\n",
      "Extracting tokens:  48%|████▊     | 268177/560000 [00:10<00:16, 18217.59it/s]\n",
      "Extracting tokens:  49%|████▊     | 272211/560000 [00:11<00:13, 21790.02it/s]\n",
      "Extracting tokens:  49%|████▉     | 276292/560000 [00:11<00:11, 25354.12it/s]\n",
      "Extracting tokens:  50%|█████     | 280372/560000 [00:11<00:09, 28594.17it/s]\n",
      "Extracting tokens:  51%|█████     | 284431/560000 [00:11<00:08, 31372.13it/s]\n",
      "Extracting tokens:  52%|█████▏    | 288466/560000 [00:11<00:08, 33603.57it/s]\n",
      "Extracting tokens:  52%|█████▏    | 292536/560000 [00:11<00:07, 35461.35it/s]\n",
      "Extracting tokens:  53%|█████▎    | 296611/560000 [00:11<00:07, 36900.60it/s]\n",
      "Extracting tokens:  54%|█████▎    | 300690/560000 [00:11<00:06, 37987.95it/s]\n",
      "Extracting tokens:  54%|█████▍    | 304759/560000 [00:11<00:06, 38758.70it/s]\n",
      "Extracting tokens:  55%|█████▌    | 308805/560000 [00:11<00:06, 38285.46it/s]\n",
      "Extracting tokens:  56%|█████▌    | 312754/560000 [00:12<00:06, 38451.89it/s]\n",
      "Extracting tokens:  57%|█████▋    | 316752/560000 [00:12<00:06, 38893.47it/s]\n",
      "Extracting tokens:  57%|█████▋    | 320838/560000 [00:12<00:06, 39467.18it/s]\n",
      "Extracting tokens:  58%|█████▊    | 324853/560000 [00:12<00:05, 39666.89it/s]\n",
      "Extracting tokens:  59%|█████▊    | 328851/560000 [00:13<00:23, 9801.38it/s] \n",
      "Extracting tokens:  59%|█████▉    | 332889/560000 [00:13<00:17, 12693.15it/s]\n",
      "Extracting tokens:  60%|██████    | 336940/560000 [00:13<00:13, 16003.76it/s]\n",
      "Extracting tokens:  61%|██████    | 340935/560000 [00:13<00:11, 19483.09it/s]\n",
      "Extracting tokens:  62%|██████▏   | 344825/560000 [00:13<00:09, 22820.53it/s]\n",
      "Extracting tokens:  62%|██████▏   | 348539/560000 [00:13<00:08, 25077.11it/s]\n",
      "Extracting tokens:  63%|██████▎   | 352357/560000 [00:14<00:07, 27905.90it/s]\n",
      "Extracting tokens:  64%|██████▎   | 356414/560000 [00:14<00:06, 30891.21it/s]\n",
      "Extracting tokens:  64%|██████▍   | 360477/560000 [00:14<00:05, 33344.39it/s]\n",
      "Extracting tokens:  65%|██████▌   | 364554/560000 [00:14<00:05, 35308.57it/s]\n",
      "Extracting tokens:  66%|██████▌   | 368653/560000 [00:14<00:05, 36864.87it/s]\n",
      "Extracting tokens:  67%|██████▋   | 372730/560000 [00:14<00:04, 37961.93it/s]\n",
      "Extracting tokens:  67%|██████▋   | 376807/560000 [00:14<00:04, 38766.10it/s]\n",
      "Extracting tokens:  68%|██████▊   | 380917/560000 [00:14<00:04, 39439.66it/s]\n",
      "Extracting tokens:  69%|██████▊   | 384994/560000 [00:14<00:04, 39827.31it/s]\n",
      "Extracting tokens:  69%|██████▉   | 389067/560000 [00:14<00:04, 40093.53it/s]\n",
      "Extracting tokens:  70%|███████   | 393131/560000 [00:15<00:04, 40236.21it/s]\n",
      "Extracting tokens:  71%|███████   | 397193/560000 [00:15<00:04, 39714.83it/s]\n",
      "Extracting tokens:  72%|███████▏  | 401274/560000 [00:15<00:03, 40037.49it/s]\n",
      "Extracting tokens:  72%|███████▏  | 405343/560000 [00:15<00:03, 40228.96it/s]\n",
      "Extracting tokens:  73%|███████▎  | 409422/560000 [00:15<00:03, 40394.43it/s]\n",
      "Extracting tokens:  74%|███████▍  | 413472/560000 [00:16<00:17, 8346.37it/s] \n",
      "Extracting tokens:  75%|███████▍  | 417546/560000 [00:16<00:12, 10965.40it/s]\n",
      "Extracting tokens:  75%|███████▌  | 421628/560000 [00:17<00:09, 14056.43it/s]\n",
      "Extracting tokens:  76%|███████▌  | 425719/560000 [00:17<00:07, 17516.21it/s]\n",
      "Extracting tokens:  77%|███████▋  | 429804/560000 [00:17<00:06, 21142.85it/s]\n",
      "Extracting tokens:  77%|███████▋  | 433894/560000 [00:17<00:05, 24732.26it/s]\n",
      "Extracting tokens:  78%|███████▊  | 437953/560000 [00:17<00:04, 27999.84it/s]\n",
      "Extracting tokens:  79%|███████▉  | 442037/560000 [00:17<00:03, 30918.54it/s]\n",
      "Extracting tokens:  80%|███████▉  | 446137/560000 [00:17<00:03, 33389.82it/s]\n",
      "Extracting tokens:  80%|████████  | 450156/560000 [00:17<00:03, 35083.28it/s]\n",
      "Extracting tokens:  81%|████████  | 454207/560000 [00:17<00:02, 36548.02it/s]\n",
      "Extracting tokens:  82%|████████▏ | 458300/560000 [00:17<00:02, 37766.16it/s]\n",
      "Extracting tokens:  83%|████████▎ | 462388/560000 [00:18<00:02, 38650.52it/s]\n",
      "Extracting tokens:  83%|████████▎ | 466478/560000 [00:18<00:02, 39299.00it/s]\n",
      "Extracting tokens:  84%|████████▍ | 470572/560000 [00:18<00:02, 39778.10it/s]\n",
      "Extracting tokens:  85%|████████▍ | 474654/560000 [00:18<00:02, 40084.15it/s]\n",
      "Extracting tokens:  85%|████████▌ | 478731/560000 [00:18<00:02, 40243.50it/s]\n",
      "Extracting tokens:  86%|████████▌ | 482821/560000 [00:18<00:01, 40436.83it/s]\n",
      "Extracting tokens:  87%|████████▋ | 486899/560000 [00:18<00:01, 40488.55it/s]\n",
      "Extracting tokens:  88%|████████▊ | 490972/560000 [00:18<00:01, 40526.97it/s]\n",
      "Extracting tokens:  88%|████████▊ | 495067/560000 [00:18<00:01, 40651.60it/s]\n",
      "Extracting tokens:  89%|████████▉ | 499144/560000 [00:18<00:01, 40619.37it/s]\n",
      "Extracting tokens:  90%|████████▉ | 503217/560000 [00:19<00:01, 40649.12it/s]\n",
      "Extracting tokens:  91%|█████████ | 507288/560000 [00:19<00:01, 40650.47it/s]\n",
      "Extracting tokens:  91%|█████████▏| 511368/560000 [00:19<00:01, 40694.09it/s]\n",
      "Extracting tokens:  92%|█████████▏| 515441/560000 [00:19<00:01, 40643.49it/s]\n",
      "Extracting tokens:  93%|█████████▎| 519508/560000 [00:21<00:05, 7082.79it/s] \n",
      "Extracting tokens:  93%|█████████▎| 523565/560000 [00:21<00:03, 9404.69it/s]\n",
      "Extracting tokens:  94%|█████████▍| 527631/560000 [00:21<00:02, 12221.69it/s]\n",
      "Extracting tokens:  95%|█████████▍| 531668/560000 [00:21<00:01, 15432.39it/s]\n",
      "Extracting tokens:  96%|█████████▌| 535690/560000 [00:21<00:01, 18905.69it/s]\n",
      "Extracting tokens:  96%|█████████▋| 539785/560000 [00:21<00:00, 22581.79it/s]\n",
      "Extracting tokens:  97%|█████████▋| 543854/560000 [00:21<00:00, 26065.47it/s]\n",
      "Extracting tokens:  98%|█████████▊| 547919/560000 [00:21<00:00, 29208.73it/s]\n",
      "Extracting tokens:  99%|█████████▊| 552001/560000 [00:21<00:00, 31941.00it/s]\n",
      "Extracting tokens:  99%|█████████▉| 556050/560000 [00:21<00:00, 34092.25it/s]\n",
      "Extracting tokens: 100%|██████████| 560000/560000 [00:22<00:00, 25383.03it/s]\n",
      "Saving preprocessing artifacts for 'dbpedia'...\n",
      "Creating datasets...\n",
      "\n",
      "Extracting tokens:   0%|          | 0/560000 [00:00<?, ?it/s]\n",
      "Extracting tokens:   1%|          | 3785/560000 [00:00<00:14, 37846.88it/s]\n",
      "Extracting tokens:   1%|▏         | 8197/560000 [00:00<00:13, 41532.25it/s]\n",
      "Extracting tokens:   2%|▏         | 12565/560000 [00:00<00:12, 42508.78it/s]\n",
      "Extracting tokens:   3%|▎         | 16951/560000 [00:00<00:12, 43038.38it/s]\n",
      "Extracting tokens:   4%|▍         | 21303/560000 [00:00<00:12, 43210.71it/s]\n",
      "Extracting tokens:   5%|▍         | 25700/560000 [00:00<00:12, 43468.43it/s]\n",
      "Extracting tokens:   5%|▌         | 30096/560000 [00:00<00:12, 43626.15it/s]\n",
      "Extracting tokens:   6%|▌         | 34468/560000 [00:00<00:12, 43653.09it/s]\n",
      "Extracting tokens:   7%|▋         | 38878/560000 [00:00<00:11, 43792.34it/s]\n",
      "Extracting tokens:   8%|▊         | 43258/560000 [00:01<00:11, 43619.59it/s]\n",
      "Extracting tokens:   9%|▊         | 47628/560000 [00:01<00:11, 43641.48it/s]\n",
      "Extracting tokens:   9%|▉         | 52010/560000 [00:01<00:11, 43693.48it/s]\n",
      "Extracting tokens:  10%|█         | 56380/560000 [00:01<00:11, 43655.48it/s]\n",
      "Extracting tokens:  11%|█         | 60746/560000 [00:01<00:11, 43548.65it/s]\n",
      "Extracting tokens:  12%|█▏        | 65101/560000 [00:01<00:11, 43510.81it/s]\n",
      "Extracting tokens:  12%|█▏        | 69478/560000 [00:01<00:11, 43587.82it/s]\n",
      "Extracting tokens:  13%|█▎        | 73837/560000 [00:01<00:11, 43554.14it/s]\n",
      "Extracting tokens:  14%|█▍        | 78193/560000 [00:01<00:11, 43549.03it/s]\n",
      "Extracting tokens:  15%|█▍        | 82548/560000 [00:01<00:10, 43535.45it/s]\n",
      "Extracting tokens:  16%|█▌        | 86902/560000 [00:02<00:10, 43272.74it/s]\n",
      "Extracting tokens:  16%|█▋        | 91236/560000 [00:02<00:10, 43292.43it/s]\n",
      "Extracting tokens:  17%|█▋        | 95582/560000 [00:02<00:10, 43341.23it/s]\n",
      "Extracting tokens:  18%|█▊        | 99961/560000 [00:02<00:10, 43474.36it/s]\n",
      "Extracting tokens:  19%|█▊        | 104316/560000 [00:02<00:10, 43495.78it/s]\n",
      "Extracting tokens:  19%|█▉        | 108692/560000 [00:02<00:10, 43572.36it/s]\n",
      "Extracting tokens:  20%|██        | 113050/560000 [00:02<00:10, 43508.00it/s]\n",
      "Extracting tokens:  21%|██        | 117451/560000 [00:02<00:10, 43655.40it/s]\n",
      "Extracting tokens:  22%|██▏       | 121843/560000 [00:02<00:10, 43734.04it/s]\n",
      "Extracting tokens:  23%|██▎       | 126224/560000 [00:02<00:09, 43754.44it/s]\n",
      "Extracting tokens:  23%|██▎       | 130645/560000 [00:03<00:09, 43886.05it/s]\n",
      "Extracting tokens:  24%|██▍       | 135034/560000 [00:03<00:09, 43872.88it/s]\n",
      "Extracting tokens:  25%|██▍       | 139422/560000 [00:03<00:09, 43843.62it/s]\n",
      "Extracting tokens:  26%|██▌       | 143807/560000 [00:03<00:24, 17302.58it/s]\n",
      "Extracting tokens:  26%|██▋       | 148193/560000 [00:03<00:19, 21140.76it/s]\n",
      "Extracting tokens:  27%|██▋       | 152591/560000 [00:04<00:16, 25048.73it/s]\n",
      "Extracting tokens:  28%|██▊       | 156951/560000 [00:04<00:14, 28692.04it/s]\n",
      "Extracting tokens:  29%|██▉       | 161357/560000 [00:04<00:12, 32058.63it/s]\n",
      "Extracting tokens:  30%|██▉       | 165705/560000 [00:04<00:11, 34780.75it/s]\n",
      "Extracting tokens:  30%|███       | 170110/560000 [00:04<00:10, 37133.38it/s]\n",
      "Extracting tokens:  31%|███       | 174473/560000 [00:04<00:09, 38861.72it/s]\n",
      "Extracting tokens:  32%|███▏      | 178818/560000 [00:04<00:09, 40124.48it/s]\n",
      "Extracting tokens:  33%|███▎      | 183123/560000 [00:05<00:25, 14859.43it/s]\n",
      "Extracting tokens:  33%|███▎      | 187465/560000 [00:05<00:20, 18506.76it/s]\n",
      "Extracting tokens:  34%|███▍      | 191828/560000 [00:05<00:16, 22382.83it/s]\n",
      "Extracting tokens:  35%|███▌      | 196199/560000 [00:05<00:13, 26234.44it/s]\n",
      "Extracting tokens:  36%|███▌      | 200585/560000 [00:05<00:12, 29848.96it/s]\n",
      "Extracting tokens:  37%|███▋      | 204972/560000 [00:05<00:10, 33025.26it/s]\n",
      "Extracting tokens:  37%|███▋      | 209381/560000 [00:05<00:09, 35727.88it/s]\n",
      "Extracting tokens:  38%|███▊      | 213758/560000 [00:06<00:09, 37808.54it/s]\n",
      "Extracting tokens:  39%|███▉      | 218165/560000 [00:06<00:08, 39496.85it/s]\n",
      "Extracting tokens:  40%|███▉      | 222536/560000 [00:06<00:08, 40659.14it/s]\n",
      "Extracting tokens:  41%|████      | 226926/560000 [00:06<00:08, 41579.94it/s]\n",
      "Extracting tokens:  41%|████▏     | 231278/560000 [00:07<00:25, 13058.68it/s]\n",
      "Extracting tokens:  42%|████▏     | 235648/560000 [00:07<00:19, 16533.18it/s]\n",
      "Extracting tokens:  43%|████▎     | 240066/560000 [00:07<00:15, 20387.39it/s]\n",
      "Extracting tokens:  44%|████▎     | 244461/560000 [00:07<00:12, 24286.40it/s]\n",
      "Extracting tokens:  44%|████▍     | 248781/560000 [00:07<00:11, 27911.01it/s]\n",
      "Extracting tokens:  45%|████▌     | 253201/560000 [00:07<00:09, 31411.60it/s]\n",
      "Extracting tokens:  46%|████▌     | 257385/560000 [00:07<00:08, 33761.09it/s]\n",
      "Extracting tokens:  47%|████▋     | 261553/560000 [00:07<00:08, 35590.75it/s]\n",
      "Extracting tokens:  47%|████▋     | 265700/560000 [00:08<00:07, 37004.81it/s]\n",
      "Extracting tokens:  48%|████▊     | 269830/560000 [00:08<00:07, 38042.71it/s]\n",
      "Extracting tokens:  49%|████▉     | 273945/560000 [00:08<00:07, 38036.00it/s]\n",
      "Extracting tokens:  50%|████▉     | 278031/560000 [00:08<00:07, 38827.57it/s]\n",
      "Extracting tokens:  50%|█████     | 282106/560000 [00:08<00:07, 39375.87it/s]\n",
      "Extracting tokens:  51%|█████     | 286177/560000 [00:08<00:06, 39760.43it/s]\n",
      "Extracting tokens:  52%|█████▏    | 290265/560000 [00:08<00:06, 40086.34it/s]\n",
      "Extracting tokens:  53%|█████▎    | 294331/560000 [00:09<00:25, 10539.09it/s]\n",
      "Extracting tokens:  53%|█████▎    | 298429/560000 [00:09<00:19, 13564.47it/s]\n",
      "Extracting tokens:  54%|█████▍    | 302500/560000 [00:09<00:15, 16939.29it/s]\n",
      "Extracting tokens:  55%|█████▍    | 306544/560000 [00:09<00:12, 20483.75it/s]\n",
      "Extracting tokens:  55%|█████▌    | 310624/560000 [00:10<00:10, 24068.34it/s]\n",
      "Extracting tokens:  56%|█████▌    | 314731/560000 [00:10<00:08, 27501.43it/s]\n",
      "Extracting tokens:  57%|█████▋    | 318814/560000 [00:10<00:07, 30483.49it/s]\n",
      "Extracting tokens:  58%|█████▊    | 322898/560000 [00:10<00:07, 32991.65it/s]\n",
      "Extracting tokens:  58%|█████▊    | 326972/560000 [00:10<00:06, 34982.62it/s]\n",
      "Extracting tokens:  59%|█████▉    | 331050/560000 [00:10<00:06, 36538.85it/s]\n",
      "Extracting tokens:  60%|█████▉    | 335086/560000 [00:10<00:05, 37598.06it/s]\n",
      "Extracting tokens:  61%|██████    | 339120/560000 [00:10<00:05, 38365.79it/s]\n",
      "Extracting tokens:  61%|██████▏   | 343153/560000 [00:10<00:05, 38753.29it/s]\n",
      "Extracting tokens:  62%|██████▏   | 347204/560000 [00:10<00:05, 39261.24it/s]\n",
      "Extracting tokens:  63%|██████▎   | 351255/560000 [00:11<00:05, 39624.83it/s]\n",
      "Extracting tokens:  63%|██████▎   | 355331/560000 [00:11<00:05, 39957.53it/s]\n",
      "Extracting tokens:  64%|██████▍   | 359444/560000 [00:11<00:04, 40302.89it/s]\n",
      "Extracting tokens:  65%|██████▍   | 363557/560000 [00:11<00:04, 40546.95it/s]\n",
      "Extracting tokens:  66%|██████▌   | 367637/560000 [00:12<00:21, 9018.14it/s] \n",
      "Extracting tokens:  66%|██████▋   | 371743/560000 [00:12<00:15, 11787.68it/s]\n",
      "Extracting tokens:  67%|██████▋   | 375864/560000 [00:12<00:12, 15021.41it/s]\n",
      "Extracting tokens:  68%|██████▊   | 379995/560000 [00:12<00:09, 18588.72it/s]\n",
      "Extracting tokens:  69%|██████▊   | 384111/560000 [00:13<00:07, 22253.24it/s]\n",
      "Extracting tokens:  69%|██████▉   | 388222/560000 [00:13<00:06, 25803.46it/s]\n",
      "Extracting tokens:  70%|███████   | 392334/560000 [00:13<00:05, 29049.61it/s]\n",
      "Extracting tokens:  71%|███████   | 396446/560000 [00:13<00:05, 31854.12it/s]\n",
      "Extracting tokens:  72%|███████▏  | 400536/560000 [00:13<00:04, 34103.27it/s]\n",
      "Extracting tokens:  72%|███████▏  | 404628/560000 [00:13<00:04, 35891.30it/s]\n",
      "Extracting tokens:  73%|███████▎  | 408715/560000 [00:13<00:04, 37247.17it/s]\n",
      "Extracting tokens:  74%|███████▎  | 412841/560000 [00:13<00:03, 38370.35it/s]\n",
      "Extracting tokens:  74%|███████▍  | 416978/560000 [00:13<00:03, 39227.36it/s]\n",
      "Extracting tokens:  75%|███████▌  | 421077/560000 [00:13<00:03, 39728.37it/s]\n",
      "Extracting tokens:  76%|███████▌  | 425215/560000 [00:14<00:03, 40209.44it/s]\n",
      "Extracting tokens:  77%|███████▋  | 429325/560000 [00:14<00:03, 40438.76it/s]\n",
      "Extracting tokens:  77%|███████▋  | 433444/560000 [00:14<00:03, 40657.78it/s]\n",
      "Extracting tokens:  78%|███████▊  | 437564/560000 [00:14<00:02, 40816.55it/s]\n",
      "Extracting tokens:  79%|███████▉  | 441677/560000 [00:14<00:02, 40868.18it/s]\n",
      "Extracting tokens:  80%|███████▉  | 445803/560000 [00:14<00:02, 40983.43it/s]\n",
      "Extracting tokens:  80%|████████  | 449917/560000 [00:14<00:02, 40924.05it/s]\n",
      "Extracting tokens:  81%|████████  | 454021/560000 [00:14<00:02, 40878.87it/s]\n",
      "Extracting tokens:  82%|████████▏ | 458148/560000 [00:14<00:02, 40993.16it/s]\n",
      "Extracting tokens:  83%|████████▎ | 462267/560000 [00:14<00:02, 41051.72it/s]\n",
      "Extracting tokens:  83%|████████▎ | 466376/560000 [00:16<00:12, 7755.69it/s] \n",
      "Extracting tokens:  84%|████████▍ | 470470/560000 [00:16<00:08, 10235.44it/s]\n",
      "Extracting tokens:  85%|████████▍ | 474607/560000 [00:16<00:06, 13238.47it/s]\n",
      "Extracting tokens:  85%|████████▌ | 478689/560000 [00:16<00:04, 16578.70it/s]\n",
      "Extracting tokens:  86%|████████▌ | 482812/560000 [00:16<00:03, 20214.49it/s]\n",
      "Extracting tokens:  87%|████████▋ | 486924/560000 [00:16<00:03, 23852.09it/s]\n",
      "Extracting tokens:  88%|████████▊ | 491032/560000 [00:17<00:02, 27281.27it/s]\n",
      "Extracting tokens:  88%|████████▊ | 495152/560000 [00:17<00:02, 30362.35it/s]\n",
      "Extracting tokens:  89%|████████▉ | 499286/560000 [00:17<00:01, 32999.71it/s]\n",
      "Extracting tokens:  90%|████████▉ | 503339/560000 [00:17<00:01, 34840.61it/s]\n",
      "Extracting tokens:  91%|█████████ | 507449/560000 [00:17<00:01, 36510.83it/s]\n",
      "Extracting tokens:  91%|█████████▏| 511559/560000 [00:17<00:01, 37776.83it/s]\n",
      "Extracting tokens:  92%|█████████▏| 515673/560000 [00:17<00:01, 38725.84it/s]\n",
      "Extracting tokens:  93%|█████████▎| 519785/560000 [00:17<00:01, 39393.55it/s]\n",
      "Extracting tokens:  94%|█████████▎| 523900/560000 [00:17<00:00, 39902.53it/s]\n",
      "Extracting tokens:  94%|█████████▍| 527999/560000 [00:17<00:00, 40131.32it/s]\n",
      "Extracting tokens:  95%|█████████▌| 532089/560000 [00:18<00:00, 40350.37it/s]\n",
      "Extracting tokens:  96%|█████████▌| 536178/560000 [00:18<00:00, 40494.49it/s]\n",
      "Extracting tokens:  96%|█████████▋| 540266/560000 [00:18<00:00, 40512.13it/s]\n",
      "Extracting tokens:  97%|█████████▋| 544365/560000 [00:18<00:00, 40653.47it/s]\n",
      "Extracting tokens:  98%|█████████▊| 548469/560000 [00:18<00:00, 40768.39it/s]\n",
      "Extracting tokens:  99%|█████████▊| 552559/560000 [00:18<00:00, 40787.01it/s]\n",
      "Extracting tokens:  99%|█████████▉| 556647/560000 [00:18<00:00, 40675.86it/s]\n",
      "Extracting tokens: 100%|██████████| 560000/560000 [00:18<00:00, 29816.88it/s]\n",
      "\n",
      "Extracting tokens:   0%|          | 0/35000 [00:00<?, ?it/s]\n",
      "Extracting tokens:  11%|█▏        | 4020/35000 [00:00<00:00, 40195.44it/s]\n",
      "Extracting tokens:  23%|██▎       | 8095/35000 [00:00<00:00, 40521.15it/s]\n",
      "Extracting tokens:  35%|███▍      | 12149/35000 [00:00<00:00, 40528.14it/s]\n",
      "Extracting tokens:  46%|████▋     | 16228/35000 [00:00<00:00, 40630.12it/s]\n",
      "Extracting tokens:  58%|█████▊    | 20312/35000 [00:00<00:00, 40704.20it/s]\n",
      "Extracting tokens:  70%|██████▉   | 24388/35000 [00:00<00:00, 40720.87it/s]\n",
      "Extracting tokens:  81%|████████▏ | 28466/35000 [00:00<00:00, 40738.85it/s]\n",
      "Extracting tokens:  93%|█████████▎| 32569/35000 [00:00<00:00, 40828.75it/s]\n",
      "Extracting tokens: 100%|██████████| 35000/35000 [00:00<00:00, 40690.75it/s]\n",
      "Creating dataloaders...\n",
      "\n",
      "Creating FLAT model...\n",
      "Model created: FlatTransformer\n",
      "  - d_model: 768\n",
      "  - num_layers: 4\n",
      "  - num_heads: 6\n",
      "✓ Task head created: ClassificationHead\n",
      "✓ Loss function created for task: classification\n",
      "\n",
      "Initializing trainer for experiment: dbpedia_flat...\n",
      "\n",
      "[DIAGNOSTIC] LR Scheduler created:\n",
      "  - Type: ReduceLROnPlateau\n",
      "  - LR Schedule: reduce_on_plateau\n",
      "  - Mode: min\n",
      "  - Patience: 3\n",
      "  - Factor: 0.5\n",
      "  - Min LR: 1e-08\n",
      "  - Mode: min\n",
      "  - Patience: 3\n",
      "  - Factor: 0.5\n",
      "  - Min LR: 1e-08\n",
      "  - Total training steps: 10000\n",
      "  - Initial LR: 1.00e-06\n",
      "\n",
      "Starting training...\n",
      "============================================================\n",
      "Starting training for 2 epochs...\n",
      "Total steps: 10000\n",
      "Device: cuda\n",
      "\n",
      "[TRAIN] Step 0:\n",
      "  train/grad_norm: 14.548221\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 38.551712\n",
      "\n",
      "[TRAIN] Step 100:\n",
      "  train/grad_norm: 13.968676\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 34.082928\n",
      "\n",
      "[TRAIN] Step 200:\n",
      "  train/grad_norm: 13.772025\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 34.066723\n",
      "\n",
      "[TRAIN] Step 300:\n",
      "  train/grad_norm: 12.463810\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 29.419594\n",
      "\n",
      "[TRAIN] Step 400:\n",
      "  train/grad_norm: 14.962444\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 24.080894\n",
      "\n",
      "[TRAIN] Step 500:\n",
      "  train/grad_norm: 11.335840\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 22.026714\n",
      "\n",
      "[TRAIN] Step 600:\n",
      "  train/grad_norm: 13.666917\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 21.702028\n",
      "\n",
      "[TRAIN] Step 700:\n",
      "  train/grad_norm: 12.576065\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 16.505484\n",
      "\n",
      "[TRAIN] Step 800:\n",
      "  train/grad_norm: 13.138810\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 14.515120\n",
      "\n",
      "[TRAIN] Step 900:\n",
      "  train/grad_norm: 11.569162\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 9.940715\n",
      "\n",
      "[TRAIN] Step 1000:\n",
      "  train/grad_norm: 10.540558\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 8.848682\n",
      "\n",
      "[TRAIN] Step 1100:\n",
      "  train/grad_norm: 9.262659\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 6.606277\n",
      "\n",
      "[TRAIN] Step 1200:\n",
      "  train/grad_norm: 5.750149\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 3.592839\n",
      "\n",
      "[TRAIN] Step 1300:\n",
      "  train/grad_norm: 3.505510\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 2.162735\n",
      "\n",
      "[TRAIN] Step 1400:\n",
      "  train/grad_norm: 1.235056\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.761414\n",
      "\n",
      "[TRAIN] Step 1500:\n",
      "  train/grad_norm: 0.989356\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.647144\n",
      "\n",
      "[TRAIN] Step 1600:\n",
      "  train/grad_norm: 1.537350\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.670883\n",
      "\n",
      "[TRAIN] Step 1700:\n",
      "  train/grad_norm: 0.874537\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.568997\n",
      "\n",
      "[TRAIN] Step 1800:\n",
      "  train/grad_norm: 0.875513\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.453947\n",
      "\n",
      "[TRAIN] Step 1900:\n",
      "  train/grad_norm: 1.681899\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.464053\n",
      "\n",
      "[TRAIN] Step 2000:\n",
      "  train/grad_norm: 0.812674\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.372476\n",
      "\n",
      "[TRAIN] Step 2100:\n",
      "  train/grad_norm: 0.752467\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.350535\n",
      "\n",
      "[TRAIN] Step 2200:\n",
      "  train/grad_norm: 1.215599\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.293561\n",
      "\n",
      "[TRAIN] Step 2300:\n",
      "  train/grad_norm: 0.799308\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.226682\n",
      "\n",
      "[TRAIN] Step 2400:\n",
      "  train/grad_norm: 1.342800\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.248949\n",
      "\n",
      "[TRAIN] Step 2500:\n",
      "  train/grad_norm: 1.362661\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.224796\n",
      "\n",
      "[TRAIN] Step 2600:\n",
      "  train/grad_norm: 1.354219\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.143642\n",
      "\n",
      "[TRAIN] Step 2700:\n",
      "  train/grad_norm: 0.821618\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.165519\n",
      "\n",
      "[TRAIN] Step 2800:\n",
      "  train/grad_norm: 0.789541\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.065252\n",
      "\n",
      "[TRAIN] Step 2900:\n",
      "  train/grad_norm: 0.807323\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.076412\n",
      "\n",
      "[TRAIN] Step 3000:\n",
      "  train/grad_norm: 0.972284\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.032254\n",
      "\n",
      "[TRAIN] Step 3100:\n",
      "  train/grad_norm: 0.927841\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 1.019919\n",
      "\n",
      "[TRAIN] Step 3200:\n",
      "  train/grad_norm: 0.706136\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.978519\n",
      "\n",
      "[TRAIN] Step 3300:\n",
      "  train/grad_norm: 1.045693\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.972908\n",
      "\n",
      "[TRAIN] Step 3400:\n",
      "  train/grad_norm: 0.930815\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.920865\n",
      "\n",
      "[TRAIN] Step 3500:\n",
      "  train/grad_norm: 1.076261\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.915797\n",
      "\n",
      "[TRAIN] Step 3600:\n",
      "  train/grad_norm: 0.805954\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.901959\n",
      "\n",
      "[TRAIN] Step 3700:\n",
      "  train/grad_norm: 0.741608\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.869868\n",
      "\n",
      "[TRAIN] Step 3800:\n",
      "  train/grad_norm: 0.644374\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.847839\n",
      "\n",
      "[TRAIN] Step 3900:\n",
      "  train/grad_norm: 0.691175\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.835121\n",
      "\n",
      "[TRAIN] Step 4000:\n",
      "  train/grad_norm: 0.915991\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.848422\n",
      "\n",
      "[TRAIN] Step 4100:\n",
      "  train/grad_norm: 0.902124\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.789679\n",
      "\n",
      "[TRAIN] Step 4200:\n",
      "  train/grad_norm: 0.825930\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.772849\n",
      "\n",
      "[TRAIN] Step 4300:\n",
      "  train/grad_norm: 0.851504\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.788089\n",
      "\n",
      "[TRAIN] Step 4400:\n",
      "  train/grad_norm: 0.767706\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.760986\n",
      "\n",
      "[TRAIN] Step 4500:\n",
      "  train/grad_norm: 0.928241\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.775695\n",
      "\n",
      "[TRAIN] Step 4600:\n",
      "  train/grad_norm: 0.662794\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.746355\n",
      "\n",
      "[TRAIN] Step 4700:\n",
      "  train/grad_norm: 0.611150\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.690556\n",
      "\n",
      "[TRAIN] Step 4800:\n",
      "  train/grad_norm: 1.119477\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.713390\n",
      "\n",
      "[TRAIN] Step 4900:\n",
      "  train/grad_norm: 0.958530\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.688006\n",
      "\n",
      "[TRAIN] Step 5000:\n",
      "  train/grad_norm: 0.535188\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.692332\n",
      "\n",
      "[TRAIN] Step 5100:\n",
      "  train/grad_norm: 1.384217\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.690869\n",
      "\n",
      "[TRAIN] Step 5200:\n",
      "  train/grad_norm: 0.554860\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.673993\n",
      "\n",
      "[TRAIN] Step 5300:\n",
      "  train/grad_norm: 0.611806\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.671506\n",
      "\n",
      "[TRAIN] Step 5400:\n",
      "  train/grad_norm: 0.958842\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.661630\n",
      "\n",
      "[TRAIN] Step 5500:\n",
      "  train/grad_norm: 0.910490\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.652840\n",
      "\n",
      "[TRAIN] Step 5600:\n",
      "  train/grad_norm: 0.629634\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.662201\n",
      "\n",
      "[TRAIN] Step 5700:\n",
      "  train/grad_norm: 0.800577\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.659181\n",
      "\n",
      "[TRAIN] Step 5800:\n",
      "  train/grad_norm: 0.631162\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.622451\n",
      "\n",
      "[TRAIN] Step 5900:\n",
      "  train/grad_norm: 0.699716\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.634023\n",
      "\n",
      "[TRAIN] Step 6000:\n",
      "  train/grad_norm: 0.836393\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.632037\n",
      "\n",
      "[TRAIN] Step 6100:\n",
      "  train/grad_norm: 0.627190\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.621752\n",
      "\n",
      "[TRAIN] Step 6200:\n",
      "  train/grad_norm: 0.932121\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.631406\n",
      "\n",
      "[TRAIN] Step 6300:\n",
      "  train/grad_norm: 0.774081\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.622890\n",
      "\n",
      "[TRAIN] Step 6400:\n",
      "  train/grad_norm: 0.741043\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.611717\n",
      "\n",
      "[TRAIN] Step 6500:\n",
      "  train/grad_norm: 0.512865\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.614139\n",
      "\n",
      "[TRAIN] Step 6600:\n",
      "  train/grad_norm: 0.658559\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.619581\n",
      "\n",
      "[TRAIN] Step 6700:\n",
      "  train/grad_norm: 0.598671\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.606405\n",
      "\n",
      "[TRAIN] Step 6800:\n",
      "  train/grad_norm: 0.710978\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.603318\n",
      "\n",
      "[TRAIN] Step 6900:\n",
      "  train/grad_norm: 0.640721\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.606197\n",
      "\n",
      "[TRAIN] Step 7000:\n",
      "  train/grad_norm: 0.753698\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.601019\n",
      "\n",
      "[TRAIN] Step 7100:\n",
      "  train/grad_norm: 0.620172\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.599337\n",
      "\n",
      "[TRAIN] Step 7200:\n",
      "  train/grad_norm: 0.657640\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.601876\n",
      "\n",
      "[TRAIN] Step 7300:\n",
      "  train/grad_norm: 0.615581\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.595130\n",
      "\n",
      "[TRAIN] Step 7400:\n",
      "  train/grad_norm: 0.620318\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.604144\n",
      "\n",
      "[TRAIN] Step 7500:\n",
      "  train/grad_norm: 0.516140\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.599961\n",
      "\n",
      "[TRAIN] Step 7600:\n",
      "  train/grad_norm: 0.514569\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.590870\n",
      "\n",
      "[TRAIN] Step 7700:\n",
      "  train/grad_norm: 0.645463\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.597707\n",
      "\n",
      "[TRAIN] Step 7800:\n",
      "  train/grad_norm: 0.618501\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.588986\n",
      "\n",
      "[TRAIN] Step 7900:\n",
      "  train/grad_norm: 0.631685\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.602004\n",
      "\n",
      "[TRAIN] Step 8000:\n",
      "  train/grad_norm: 0.619019\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.597140\n",
      "\n",
      "[TRAIN] Step 8100:\n",
      "  train/grad_norm: 0.512540\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.596221\n",
      "\n",
      "[TRAIN] Step 8200:\n",
      "  train/grad_norm: 0.642591\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.606194\n",
      "\n",
      "[TRAIN] Step 8300:\n",
      "  train/grad_norm: 0.494550\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.587789\n",
      "\n",
      "[TRAIN] Step 8400:\n",
      "  train/grad_norm: 0.577934\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.592666\n",
      "\n",
      "[TRAIN] Step 8500:\n",
      "  train/grad_norm: 0.613637\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.604241\n",
      "\n",
      "[TRAIN] Step 8600:\n",
      "  train/grad_norm: 0.800032\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.598651\n",
      "\n",
      "[TRAIN] Step 8700:\n",
      "  train/grad_norm: 0.818187\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.596446\n",
      "\n",
      "============================================================\n",
      "[TRAIN] Epoch 0:\n",
      "  train/loss: 3.521836\n",
      "============================================================\n",
      "\n",
      "[DIAGNOSTIC] Validation batch losses (first 10 batches):\n",
      "  Batch 0: loss = 0.557763\n",
      "  Batch 1: loss = 0.558806\n",
      "  Batch 2: loss = 0.559229\n",
      "  Batch 3: loss = 0.557442\n",
      "  Batch 4: loss = 0.557491\n",
      "  Batch 5: loss = 0.558495\n",
      "  Batch 6: loss = 0.559011\n",
      "  Batch 7: loss = 0.558599\n",
      "  Batch 8: loss = 0.558560\n",
      "  Batch 9: loss = 0.557151\n",
      "  ... (537 more batches)\n",
      "\n",
      "[DIAGNOSTIC] Validation at epoch 0:\n",
      "  - Batches: 547\n",
      "  - Approximate size: 35008\n",
      "  - Validation loss: 0.557762\n",
      "  - Training loss (epoch avg): 3.521836\n",
      "  - Loss gap: 2.964074 (84.16%)\n",
      "  - ⚠️  WARNING: Val loss is much lower than train loss!\n",
      "\n",
      "============================================================\n",
      "[VAL] Epoch 0:\n",
      "  val/loss: 0.557762\n",
      "============================================================\n",
      "  LR after plateau check: 1.00e-06\n",
      "\n",
      "[TRAIN] Step 8800:\n",
      "  train/grad_norm: 0.551075\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.591162\n",
      "\n",
      "[TRAIN] Step 8900:\n",
      "  train/grad_norm: 0.509541\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.588584\n",
      "\n",
      "[TRAIN] Step 9000:\n",
      "  train/grad_norm: 0.470882\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.589894\n",
      "\n",
      "[TRAIN] Step 9100:\n",
      "  train/grad_norm: 0.466415\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.586191\n",
      "\n",
      "[TRAIN] Step 9200:\n",
      "  train/grad_norm: 0.471245\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.588208\n",
      "\n",
      "[TRAIN] Step 9300:\n",
      "  train/grad_norm: 0.898145\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.607230\n",
      "\n",
      "[TRAIN] Step 9400:\n",
      "  train/grad_norm: 0.691621\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.598610\n",
      "\n",
      "[TRAIN] Step 9500:\n",
      "  train/grad_norm: 0.823675\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.601430\n",
      "\n",
      "[TRAIN] Step 9600:\n",
      "  train/grad_norm: 0.507082\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.588363\n",
      "\n",
      "[TRAIN] Step 9700:\n",
      "  train/grad_norm: 0.661179\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.595339\n",
      "\n",
      "[TRAIN] Step 9800:\n",
      "  train/grad_norm: 0.449699\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.591984\n",
      "\n",
      "[TRAIN] Step 9900:\n",
      "  train/grad_norm: 0.522280\n",
      "  train/learning_rate: 1.00e-06\n",
      "  train/loss: 0.588558\n",
      "\n",
      "============================================================\n",
      "[TRAIN] Epoch 1:\n",
      "  train/loss: 0.593635\n",
      "============================================================\n",
      "\n",
      "[DIAGNOSTIC] Validation at epoch 1:\n",
      "  - Batches: 547\n",
      "  - Approximate size: 35008\n",
      "  - Validation loss: 0.555629\n",
      "  - Training loss (epoch avg): 0.593635\n",
      "  - Loss gap: 0.038006 (6.40%)\n",
      "\n",
      "============================================================\n",
      "[VAL] Epoch 1:\n",
      "  val/loss: 0.555629\n",
      "============================================================\n",
      "  LR after plateau check: 1.00e-06\n",
      "Reached max_steps (10000), stopping training.\n",
      "============================================================\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Training Summary:\n",
      "  - Total epochs: 2\n",
      "  - Final train loss: 0.593635\n",
      "  - Final val loss: 0.555629\n",
      "  - Checkpoints saved to: checkpoints/dbpedia_flat\n",
      "  - Logs saved to: logs/dbpedia_flat\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    \"-u\",\n",
    "    \"-m\",\n",
    "    \"saab_v3.train\",\n",
    "    \"--dataset\",\n",
    "    str(dataset_name),\n",
    "    \"--model\",\n",
    "    str(model_type),\n",
    "    \"--experiment-name\",\n",
    "    str(experiment_name),\n",
    "]\n",
    "\n",
    "if resume_checkpoint:\n",
    "    cmd += [\"--resume\", str(resume_checkpoint)]\n",
    "\n",
    "p = subprocess.Popen(\n",
    "    [\"stdbuf\", \"-oL\", \"-eL\", *cmd],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    "    bufsize=1,\n",
    ")\n",
    "\n",
    "for line in p.stdout:\n",
    "    print(line, end=\"\")\n",
    "\n",
    "p.wait()\n",
    "if p.returncode:\n",
    "    raise RuntimeError(f\"exit code {p.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate trained models on test/validation data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Evaluation Configuration\n",
    "\n",
    "Edit these variables to configure your evaluation run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation configuration\n",
    "checkpoint_path = \"/content/drive/MyDrive/SAAB/checkpoints/dbpedia_flat/best_model.pt\"  # Path to checkpoint file\n",
    "eval_dataset_name = \"dbpedia\"  # Dataset identifier (usually same as training)\n",
    "eval_split = \"test\"  # \"val\" or \"test\"\n",
    "eval_batch_size = 64  # Batch size for evaluation\n",
    "\n",
    "# Display configuration\n",
    "print(\"Evaluation Configuration:\")\n",
    "print(f\"  Checkpoint: {checkpoint_path}\")\n",
    "print(f\"  Dataset: {eval_dataset_name}\")\n",
    "print(f\"  Split: {eval_split}\")\n",
    "print(f\"  Batch Size: {eval_batch_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Run Evaluation\n",
    "\n",
    "This cell executes the evaluation CLI command. Output will be displayed in real-time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    \"-u\",\n",
    "    \"-m\",\n",
    "    \"saab_v3.evaluate\",\n",
    "    \"--checkpoint\",\n",
    "    str(checkpoint_path),\n",
    "    \"--dataset-name\",\n",
    "    str(eval_dataset_name),\n",
    "    \"--split\",\n",
    "    str(eval_split),\n",
    "    \"--device\",\n",
    "    \"cuda\",\n",
    "    \"--batch-size\",\n",
    "    str(eval_batch_size),\n",
    "]\n",
    "\n",
    "p = subprocess.Popen(\n",
    "    [\"stdbuf\", \"-oL\", \"-eL\", *cmd],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    "    bufsize=1,\n",
    ")\n",
    "\n",
    "for line in p.stdout:\n",
    "    print(line, end=\"\")\n",
    "\n",
    "p.wait()\n",
    "if p.returncode:\n",
    "    raise RuntimeError(f\"exit code {p.returncode}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
