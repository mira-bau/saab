# Default training configuration
# This file can be used as a base for training experiments

training:
  # Optimizer settings
  optimizer_type: "adamw"  # "adam" or "adamw"
  learning_rate: 1e-4
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8

  # Learning rate schedule
  lr_schedule: "linear_warmup_cosine"  # "constant", "linear_warmup", "linear_warmup_cosine", "linear_warmup_polynomial"
  warmup_steps: 1000
  # warmup_ratio: null  # Alternative to warmup_steps (ratio of total steps)
  min_lr_ratio: 0.0  # Minimum LR as ratio of initial LR

  # Training settings
  batch_size: 32
  num_epochs: 10
  # max_steps: null  # Alternative to num_epochs
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0  # Gradient clipping (0 = no clipping)
  seed: 42  # For reproducibility

  # Checkpointing
  save_dir: null  # null = auto (checkpoints/{experiment_name}/)
  save_steps: null  # Save every N steps (null = disabled)
  save_epochs: 1  # Save every N epochs (null = disabled)
  keep_checkpoints: 3  # Keep last N checkpoints
  save_best: true  # Save best model based on validation metric
  best_metric: "loss"  # Metric to track for best model
  best_mode: "min"  # "min" or "max"

  # Logging
  log_dir: null  # null = auto (logs/{experiment_name}/)
  log_steps: 100  # Log every N steps
  log_epochs: true  # Log at end of each epoch
  use_tensorboard: false
  use_wandb: false
  wandb_project: null
  wandb_run_name: null

  # Validation
  eval_steps: null  # Evaluate every N steps (null = disabled)
  eval_epochs: 1  # Evaluate every N epochs
  eval_metrics: ["loss"]  # Metrics to compute during validation

  # Device
  device: "auto"  # "cpu", "cuda", "mps", "auto"

